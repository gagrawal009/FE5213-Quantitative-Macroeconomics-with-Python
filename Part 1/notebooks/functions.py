import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import torch
from torch.utils.data import DataLoader


def scatter_plot(x_: np.ndarray,
                 y_: np.ndarray,
                 name_x: str,
                 name_y: str,
                 ax=None):
    """
    scatter plot
    """
    data = pd.DataFrame(np.concatenate([x_, y_], axis=1),
                    columns=[name_x, name_y])
    if ax is None:
        sns.jointplot(data=data, x=name_x, y=name_y, kind="reg")
    else:
        sns.jointplot(data=data, x=name_x, y=name_y, kind="reg", ax=ax)
        

def performance_metrics_of_regression(labels: np.ndarray,
                                      predictions: np.ndarray) -> tuple:
    """
    Compute standard performance metrics for regression: mse and Rsquared
    """
    mse = ((predictions - labels) ** 2).mean()
    r2 = 1 - mse / (labels ** 2).mean()
    print(f'r2 = {"%.2f" % r2}, mse = {"%.2f" % mse}')

    return mse, r2


def ridge_regr(signals: np.ndarray,
                labels: np.ndarray,
                future_signals: np.ndarray,
                shrinkage_list: np.ndarray):
    """
    Regression is
    beta = (zI + S'S/t)^{-1}S'y/t = S' (zI+SS'/t)^{-1}y/t
    Inverting matrices is costly, so we use eigenvalue decomposition:
    (zI+A)^{-1} = U (zI+D)^{-1} U' where UDU' = A is eigenvalue decomposition,
    and we use the fact that D @ B = (diag(D) * B) for diagonal D, which saves a lot of compute cost
    :param signals: S
    :param labels: y
    :param future_signals: out of sample y
    :param shrinkage_list: list of ridge parameters
    :return:
    """
    t_ = signals.shape[0]
    p_ = signals.shape[1]
    if p_ < t_:
        # this is standard regression
        eigenvalues, eigenvectors = np.linalg.eigh(signals.T @ signals / t_)
        means = signals.T @ labels.reshape(-1, 1) / t_
        multiplied = eigenvectors.T @ means
        intermed = np.concatenate([(1 / (eigenvalues.reshape(-1, 1) + z)) * multiplied for z in shrinkage_list],
                                  axis=1)
        betas = eigenvectors @ intermed
    else:
        # this is the weird over-parametrized regime
        eigenvalues, eigenvectors = np.linalg.eigh(signals @ signals.T / t_)
        means = labels.reshape(-1, 1) / t_
        multiplied = eigenvectors.T @ means # this is \mu

        # now we build [(z_1+\delta)^{-1}, \cdots, (z_K+\delta)^{-1}] * \mu
        intermed = np.concatenate([(1 / (eigenvalues.reshape(-1, 1) + z)) * multiplied for z in shrinkage_list],
                                  axis=1)

        tmp = eigenvectors.T @ signals # U.T @ S
        betas = tmp.T @ intermed # (S.T @ U) @ [(z_1+\delta)^{-1}, \cdots, (z_K+\delta)^{-1}] * \mu
    predictions = future_signals @ betas
    return betas, predictions

def normalize(data: np.ndarray,
              ready_normalization: dict = None,
              method: str = 'min-max') -> tuple:
    """
    Normalize the data using the specified method.
    
    Parameters:
    - data: np.ndarray, the data to be normalized
    - ready_normalization: dict, precomputed normalization parameters (optional)
    - method: str, normalization method ('z-score', 'min-max', 'robust', 'log')
    
    Returns:
    - normalized data: np.ndarray
    - normalization parameters: dict
    """
    
    if ready_normalization is None:
        data_std = data.std(0)
        data_mean = data.mean(0)
        data_max = np.max(data, axis=0)
        data_min = np.min(data, axis=0)
        data_median = np.median(data, axis=0)
        data_iqr = np.percentile(data, 75, axis=0) - np.percentile(data, 25, axis=0)
        
        if method == 'z-score':
            data = (data - data_mean) / data_std  # this is z-scoring of the data
        elif method == 'min-max':
            data = (data - data_min) / (data_max - data_min)
            data = data - 0.5
        elif method == 'robust':
            data = (data - data_median) / data_iqr
        elif method == 'log':
            data = np.log1p(data)  # log transformation
            
    else:
        data_std = ready_normalization.get('std')
        data_mean = ready_normalization.get('mean')
        data_max = ready_normalization.get('max')
        data_min = ready_normalization.get('min')
        data_median = ready_normalization.get('median')
        data_iqr = ready_normalization.get('iqr')
        
        if method == 'z-score':
            data = (data - data_mean) / data_std  # this is z-scoring of the data
        elif method == 'min-max':
            data = (data - data_min) / (data_max - data_min)
            data = data - 0.5
        elif method == 'robust':
            data = (data - data_median) / data_iqr
        elif method == 'log':
            data = np.log1p(data)  # log transformation
    
    normalization = {'std': data_std,
                     'mean': data_mean,
                     'max': data_max,
                     'min': data_min,
                     'median': data_median,
                     'iqr': data_iqr}
    return data, normalization

def sharpe_ratio(x):
  # We are computing the ANNUALIZED SHARPE RATIO, hence we need to multiply by sqrt(12)
  return np.round(np.sqrt(12) * x.mean(0) / x.std(0), 2)

def regression_with_tstats(predicted_variable, explanatory_variables):
    x_ = explanatory_variables
    x_ = sm.add_constant(x_)
    y_ = predicted_variable
    # Newey-West standard errors with maxlags
    z_ = x_.copy().astype(float)
    result = sm.OLS(y_.values, z_.values).fit(cov_type='HAC', cov_kwds={'maxlags': 10})
    try:
        tstat = np.round(result.summary2().tables[1]['z'], 1)  # alpha t-stat (because for 'const')
        tstat.index = list(z_.columns)
    except:
        print(f'something is wrong for t-stats')
    return tstat
  
def train_model(num_epochs: int,
                train_loader: DataLoader,
                criterion,
                optimizer,
                model,
                ridge_penalty: float = 0.001):
  # Training loop
  for epoch in range(num_epochs):
      for inputs, targets in train_loader:
          # within each epoch, we just loop through mini batches, each such loop is just one pass through the whole dataset
          # Forward pass
          outputs = model(inputs)
          loss = criterion(outputs, targets) + ridge_penalty * sum(p.abs().pow(2.).sum() for p in model.parameters())

          # Backward and optimize
          optimizer.zero_grad() # kill old gradients
          loss.backward() # compute new gradients
          optimizer.step() # perform the step of gradient descent

      if (epoch+1) % 20 == 0:
          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
          
          
# Ensure the model is in evaluation mode
# Function to get predictions
def get_predictions(loader, model):
    model.eval()
    targets = []
    predictions = []

    with torch.no_grad():
        for inputs, labels in loader:
            outputs = model(inputs)
            targets.extend(labels.numpy())
            predictions.extend(outputs.numpy())

    return np.array(targets).flatten(), np.array(predictions).flatten()


